# This is a sample config file whose sole purpose is to enumerate
# all the available configuration options, and is intended to be used
# solely as a reference. Please copy this file to create a config.

# Server configuration
[server]
port = 8080
host = "127.0.0.1"
# This is the grace time (in seconds) given to the actix-server to stop the execution
# For more details: https://actix.rs/docs/server/#graceful-shutdown
shutdown_timeout = 30
# HTTP Request body limit. Defaults to 16kB
request_body_limit = 16_384
# Proxy server configuration for connecting to payment gateways.
# Don't define the fields if a Proxy isn't needed. Empty strings will cause failure.
[proxy]
# http_url = "http proxy url"
# https_url = "https proxy url"

# Main SQL data store credentials
[master_database]
username = "db_user"      # DB Username
password = "db_pass"      # DB Password
host = "localhost"        # DB Host
port = 5432               # DB Port
dbname = "hyperswitch_db" # Name of Database
pool_size = 5             # Number of connections to keep open
connection_timeout = 10   # Timeout for database connection in seconds

# Replica SQL data store credentials
[replica_database]
username = "replica_user" # DB Username
password = "replica_pass" # DB Password
host = "localhost"        # DB Host
port = 5432               # DB Port
dbname = "hyperswitch_db" # Name of Database
pool_size = 5             # Number of connections to keep open
connection_timeout = 10   # Timeout for database connection in seconds

# Redis credentials
[redis]
host = "127.0.0.1"
port = 6379
pool_size = 5              # Number of connections to keep open
reconnect_max_attempts = 5 # Maximum number of reconnection attempts to make before failing. Set to 0 to retry forever.
reconnect_delay = 5        # Delay between reconnection attempts, in milliseconds
default_ttl = 300          # Default TTL for entries, in seconds
default_hash_ttl = 900     # Default TTL for hashes entries, in seconds
use_legacy_version = false # Resp protocol for fred crate (set this to true if using RESPv2 or redis version < 6)
stream_read_count = 1      # Default number of entries to read from stream if not provided in stream read options

# Logging configuration. Logging can be either to file or console or both.

# Logging configuration for file logging
[log.file]
enabled = false         # Toggle [true or false]
path = "logs"           # specify the directory to create log files
file_name = "debug.log" # base name for log files.
# levels can be "TRACE", "DEBUG", "INFO", "WARN", "ERROR", "OFF"
# defaults to "WARN"
level = "WARN"

# Logging configuration for console logging
[log.console]
enabled = true         # boolean [true or false]
log_format = "default" # Log format. "default" or "json"
# levels can be "TRACE", "DEBUG", "INFO", "WARN", "ERROR", "OFF"
# defaults to "WARN"
level = "DEBUG"

# Telemetry configuration for traces
[log.telemetry]
enabled = false     # boolean [true or false]
sampling_rate = 0.1 # decimal rate between 0.0 - 1.0

# This section provides some secret values.
[secrets]
admin_api_key = "test_admin" # admin API key for admin authentication
jwt_secret = "secret"        # JWT secret used for user authentication

# Locker settings contain details for accessing a card locker, a
# PCI Compliant storage entity which stores payment method information
# like card details
[locker]
host = ""          # Locker host
mock_locker = true # Emulate a locker locally using Postgres
basilisk_host = "" #Basilisk host

[jwekey] # 4 priv/pub key pair
locker_key_identifier1 = "" # key identifier for key rotation , should be same as basilisk
locker_key_identifier2 = "" # key identifier for key rotation , should be same as basilisk
locker_encryption_key1 = "" # public key 1 in pem format, corresponding private key in basilisk
locker_encryption_key2 = "" # public key 2 in pem format, corresponding private key in basilisk
locker_decryption_key1 = "" # private key 1 in pem format, corresponding public key in basilisk
locker_decryption_key2 = "" # private key 2 in pem format, corresponding public key in basilisk


# Refund configuration
[refund]
max_attempts = 10 # Number of refund attempts allowed
max_age = 365     # Max age of a refund in days.

[webhooks]
outgoing_enabled = true

# Validity of an Ephemeral Key in Hours
[eph_key]
validity = 1

# Connector configuration, provided attributes will be used to fulfill API requests.
# Examples provided here are sandbox/test base urls, can be replaced by live or mock
# base urls based on your need.
# Note: These are not optional attributes. hyperswitch request can fail due to invalid/empty values.
[connectors.aci]
base_url = "https://eu-test.oppwa.com/"

[connectors.adyen]
base_url = "https://checkout-test.adyen.com/"

[connectors.authorizedotnet]
base_url = "https://apitest.authorize.net/xml/v1/request.api"

[connectors.checkout]
base_url = "https://api.sandbox.checkout.com/"

[connectors.stripe]
base_url = "https://api.stripe.com/"

[connectors.braintree]
base_url = "https://api.sandbox.braintreegateway.com/"

[connectors.klarna]
base_url = "https://api-na.playground.klarna.com/"

[connectors.applepay]
base_url = "https://apple-pay-gateway.apple.com/"

[connectors.cybersource]
base_url = "https://apitest.cybersource.com/"

[connectors.shift4]
base_url = "https://api.shift4.com/"

[connectors.rapyd]
base_url = "https://sandboxapi.rapyd.net"

[connectors.fiserv]
base_url = "https://cert.api.fiservapps.com/"

[connectors.worldpay]
base_url = "https://try.access.worldpay.com/"

[connectors.globalpay]
base_url = "https://apis.sandbox.globalpay.com/ucp/"

# This data is used to call respective connectors for wallets and cards
[connectors.dlocal]
base_url = "https://sandbox.dlocal.com/"

[connectors.supported]
wallets = ["klarna", "braintree", "applepay"]
cards = [
    "stripe",
    "adyen",
    "authorizedotnet",
    "checkout",
    "braintree",
    "cybersource",
    "shift4",
    "worldpay",
    "globalpay",
]

# Scheduler settings provides a point to modify the behaviour of scheduler flow.
# It defines the the streams/queues name and configuration as well as event selection variables
[scheduler]
stream = "SCHEDULER_STREAM"

[scheduler.consumer]
consumer_group = "SCHEDULER_GROUP"
disabled = false                   # This flag decides if the consumer should actively consume task

[scheduler.producer]
upper_fetch_limit = 0             # Upper limit for fetching entries from the redis queue (in seconds)
lower_fetch_limit = 1800          # Lower limit for fetching entries from redis queue (in seconds)
lock_key = "PRODUCER_LOCKING_KEY" # The following keys defines the producer lock that is created in redis with
lock_ttl = 160                    # the ttl being the expiry (in seconds)

batch_size = 200 # Specifies the batch size the producer will push under a single entry in the redis queue

# Drainer configuration, which handles draining raw SQL queries from Redis streams to the SQL database
[drainer]
stream_name = "DRAINER_STREAM" # Specifies the stream name to be used by the drainer
num_partitions = 64            # Specifies the number of partitions the stream will be divided into
max_read_count = 100           # Specifies the maximum number of entries that would be read from redis stream in one call
shutdown_interval = 1000       # Specifies how much time to wait, while waiting for threads to complete execution (in milliseconds)
loop_interval = 500            # Specifies how much time to wait after checking all the possible streams in completed (in milliseconds)
